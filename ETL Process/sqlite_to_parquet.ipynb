{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SQLite to Parquet\n",
                "\n",
                "implements a fully automated, RAM-safe strategies to migrate a large SQLite database from Google Drive to a Hugging Face Parquet Dataset.\n",
                "\n",
                "### Features\n",
                "- **Smart Source Selection**: Uses `drive.mount` if you know the file path, OR `gdown` if you just have a shared link.\n",
                "- **Fast Access**: Downloads/Copies to Colab VM (never to your local PC).\n",
                "- **Smart Detection**: Automatically finds the largest table to convert.\n",
                "- **DuckDB**: Fast, low-memory conversion."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install duckdb huggingface_hub gdown"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import shutil\n",
                "import json\n",
                "import duckdb\n",
                "import gdown\n",
                "from google.colab import drive\n",
                "from huggingface_hub import HfApi, login\n",
                "\n",
                "# ==========================================\n",
                "# CONFIGURATION\n",
                "# ==========================================\n",
                "config = {\n",
                "    \"source_url\": \"\",\n",
                "    \"google_drive_path\": \"\",\n",
                "    \"hf_repo_id\": \"YOUR_USERNAME/dataset-name\",\n",
                "    \"hf_token\": \"hf_YOUR_WRITE_TOKEN\"\n",
                "}\n",
                "\n",
                "# Load from file if uploaded\n",
                "config_path = \"migration_config.json\"\n",
                "if os.path.exists(config_path):\n",
                "    print(f\"Found {config_path}, loading values...\")\n",
                "    with open(config_path, 'r') as f:\n",
                "        file_config = json.load(f)\n",
                "        config.update(file_config)\n",
                "else:\n",
                "    print(\"Using in-script placeholders.\")\n",
                "\n",
                "print(f\"Target Repo: {config['hf_repo_id']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Ingestion\n",
                "Automatically determines usage of Drive Mount vs Gdown based on config."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "local_db = \"local_input.db\"\n",
                "source_path = config.get(\"google_drive_path\", \"\")\n",
                "source_url = config.get(\"source_url\", \"\")\n",
                "\n",
                "# STRATEGY 1: CHECK GOOGLE DRIVE PATH\n",
                "if source_path and \"/content/drive\" in source_path and \"YOUR_DB_FILE\" not in source_path:\n",
                "    print(\"Strategy: Google Drive System Mount\")\n",
                "    if not os.path.exists('/content/drive'):\n",
                "        drive.mount('/content/drive')\n",
                "    \n",
                "    if os.path.exists(source_path):\n",
                "        print(f\"Copying database from Drive to Local VM... (Please wait)\")\n",
                "        shutil.copy(source_path, local_db)\n",
                "        print(\"Copy complete.\")\n",
                "    else:\n",
                "        print(f\"File not found at {source_path}. Falling back to URL...\")\n",
                "\n",
                "# STRATEGY 2: CHECK GDOWN URL\n",
                "if not os.path.exists(local_db) and source_url and \"drive.google.com\" in source_url:\n",
                "    print(\"Strategy: Gdown (Direct Download to Colab)\")\n",
                "    # Extract ID for safety\n",
                "    file_id = source_url.split('/d/')[1].split('/')[0] if '/d/' in source_url else None\n",
                "    if file_id:\n",
                "        url = f'https://drive.google.com/uc?id={file_id}'\n",
                "        gdown.download(url, local_db, quiet=False)\n",
                "    else:\n",
                "        gdown.download(source_url, local_db, quiet=False, fuzzy=True)\n",
                "    \n",
                "    if os.path.exists(local_db):\n",
                "        print(\"Download complete.\")\n",
                "    else:\n",
                "         print(\"Download failed.\")\n",
                "\n",
                "if not os.path.exists(local_db):\n",
                "    raise FileNotFoundError(\"Check your config! Could not find DB via Path OR URL.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Smart Table Detection\n",
                "Scans the database and selects the table with the most rows automatically."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "con = duckdb.connect(local_db)\n",
                "\n",
                "tables = con.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';\").fetchall()\n",
                "table_names = [t[0] for t in tables]\n",
                "\n",
                "print(f\"Found tables: {table_names}\")\n",
                "\n",
                "best_table = None\n",
                "max_rows = 0\n",
                "\n",
                "# Check row counts to find the \"Main\" table\n",
                "for table in table_names:\n",
                "    try:\n",
                "        count = con.execute(f\"SELECT COUNT(*) FROM {table}\").fetchone()[0]\n",
                "        print(f\"   - Table '{table}': {count} rows\")\n",
                "        if count > max_rows:\n",
                "            max_rows = count\n",
                "            best_table = table\n",
                "    except Exception as e:\n",
                "        print(f\"   - Error reading '{table}': {e}\")\n",
                "\n",
                "if not best_table:\n",
                "    con.close()\n",
                "    raise ValueError(\"No data found in any table!\")\n",
                "\n",
                "print(f\"Auto-selected largest table: '{best_table}'\")\n",
                "con.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Conversion & Upload\n",
                "Converts the selected table to Parquet and uploads it to Hugging Face."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "parquet_file = \"train.parquet\"\n",
                "con = duckdb.connect(local_db)\n",
                "\n",
                "print(f\"Converting '{best_table}' to Parquet...\")\n",
                "con.execute(f\"\"\"\n",
                "    COPY (SELECT * FROM {best_table}) \n",
                "    TO '{parquet_file}' \n",
                "    (FORMAT 'PARQUET', CODEC 'SNAPPY');\n",
                "\"\"\")\n",
                "con.close()\n",
                "print(\"Conversion Complete.\")\n",
                "\n",
                "print(\"Uploading to Hugging Face...\")\n",
                "\n",
                "if \"OPTIONAL\" in config[\"hf_token\"] or \"YOUR_TOKEN\" in config[\"hf_token\"]:\n",
                "    print(\"Logging in interactively (Token not in config)...\")\n",
                "    login()\n",
                "else:\n",
                "    login(token=config[\"hf_token\"])\n",
                "\n",
                "api = HfApi()\n",
                "repo_id = config[\"hf_repo_id\"]\n",
                "\n",
                "print(f\"Ensuring repo {repo_id} exists...\")\n",
                "api.create_repo(repo_id=repo_id, repo_type=\"dataset\", private=True, exist_ok=True)\n",
                "\n",
                "print(f\"Uploading {parquet_file}...\")\n",
                "try:\n",
                "    api.upload_file(\n",
                "        path_or_fileobj=parquet_file,\n",
                "        path_in_repo=\"data/train.parquet\",\n",
                "        repo_id=repo_id,\n",
                "        repo_type=\"dataset\"\n",
                "    )\n",
                "    print(\"Upload Success!\")\n",
                "    print(f\"DONE! Dataset is live at: https://huggingface.co/datasets/{repo_id}\")\n",
                "except Exception as e:\n",
                "    print(f\"Upload failed: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
