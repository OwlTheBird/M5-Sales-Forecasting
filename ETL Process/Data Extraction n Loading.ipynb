{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# M5 Data ELT Pipeline (Google Colab Optimized)\n",
                "\n",
                "**Memory-Efficient Design:**\n",
                "- Uses Parquet as intermediate disk storage (avoids OOM errors)\n",
                "- Processes data in chunks from disk to SQLite\n",
                "- Deletes temp files after completion\n",
                "\n",
                "**Steps:**\n",
                "1. Install PySpark and setup\n",
                "2. Load and transform data with Spark\n",
                "3. Write to Parquet (disk-based, memory-safe)\n",
                "4. Read Parquet chunks and write to SQLite\n",
                "5. Cleanup and verify"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 0: Setup (Colab-specific)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install pyspark pyarrow -q\n",
                "print(\"Packages installed successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "\n",
                "import os\n",
                "os.chdir('/content/drive/MyDrive/m5-forecasting-accuracy/ELT')\n",
                "print(f\"Working directory: {os.getcwd()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.types import *\n",
                "from pyspark.sql import functions as F\n",
                "import pyarrow.parquet as pq\n",
                "import pandas as pd\n",
                "import sqlite3\n",
                "import shutil\n",
                "import gc\n",
                "import os\n",
                "\n",
                "# Define all paths\n",
                "script_dir = os.getcwd()\n",
                "PATH_SALES = os.path.join(script_dir, \"Raw Data\", \"sales_train_validation.csv\")\n",
                "PATH_CAL = os.path.join(script_dir, \"Raw Data\", \"calendar.csv\")\n",
                "PATH_PRICES = os.path.join(script_dir, \"Raw Data\", \"sell_prices.csv\")\n",
                "DB_PATH = os.path.join(script_dir, \"database.db\")\n",
                "PARQUET_PATH = \"/content/temp_parquet\"\n",
                "\n",
                "print(\"Paths configured:\")\n",
                "print(f\"  Sales: {PATH_SALES}\")\n",
                "print(f\"  Calendar: {PATH_CAL}\")\n",
                "print(f\"  Prices: {PATH_PRICES}\")\n",
                "print(f\"  Database: {DB_PATH}\")\n",
                "print(f\"  Temp Parquet: {PARQUET_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Prepare Fresh Database"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Preparing fresh database...\")\n",
                "if os.path.exists(DB_PATH):\n",
                "    os.remove(DB_PATH)\n",
                "    print(\"  -> Deleted existing database\")\n",
                "\n",
                "conn = sqlite3.connect(DB_PATH)\n",
                "cursor = conn.cursor()\n",
                "cursor.execute(\"\"\"\n",
                "    CREATE TABLE sales_data (\n",
                "        store_id TEXT,\n",
                "        item_id TEXT,\n",
                "        wm_yr_wk INTEGER,\n",
                "        d TEXT,\n",
                "        id TEXT,\n",
                "        dept_id TEXT,\n",
                "        cat_id TEXT,\n",
                "        state_id TEXT,\n",
                "        sales INTEGER,\n",
                "        wday INTEGER,\n",
                "        month INTEGER,\n",
                "        year INTEGER,\n",
                "        event_name_1 TEXT,\n",
                "        event_type_1 TEXT,\n",
                "        snap_CA INTEGER,\n",
                "        snap_TX INTEGER,\n",
                "        snap_WI INTEGER,\n",
                "        sell_price REAL,\n",
                "        PRIMARY KEY (store_id, item_id, d)\n",
                "    )\n",
                "\"\"\")\n",
                "conn.commit()\n",
                "conn.close()\n",
                "print(\"  -> Fresh database created with PRIMARY KEY (store_id, item_id, d)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Initialize Spark with Optimized Settings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "spark = SparkSession.builder \\\n",
                "    .appName(\"M5_Data_Prep\") \\\n",
                "    .config(\"spark.driver.memory\", \"10g\") \\\n",
                "    .config(\"spark.executor.memory\", \"4g\") \\\n",
                "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
                "    .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n",
                "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
                "    .config(\"spark.local.dir\", \"/content/spark_temp\") \\\n",
                "    .getOrCreate()\n",
                "\n",
                "print(f\"Spark session initialized!\")\n",
                "print(f\"Spark version: {spark.version}\")\n",
                "print(f\"Driver memory: 10g\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Define Schemas and Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define schemas for type safety and performance\n",
                "sales_schema = StructType([\n",
                "    StructField(\"id\", StringType(), True),\n",
                "    StructField(\"item_id\", StringType(), True),\n",
                "    StructField(\"dept_id\", StringType(), True),\n",
                "    StructField(\"cat_id\", StringType(), True),\n",
                "    StructField(\"store_id\", StringType(), True),\n",
                "    StructField(\"state_id\", StringType(), True)\n",
                "] + [StructField(f\"d_{i}\", IntegerType(), True) for i in range(1, 1914)])\n",
                "\n",
                "calendar_schema = StructType([\n",
                "    StructField(\"date\", DateType(), True),\n",
                "    StructField(\"wm_yr_wk\", ShortType(), True),\n",
                "    StructField(\"weekday\", StringType(), True),\n",
                "    StructField(\"wday\", ByteType(), True),\n",
                "    StructField(\"month\", ByteType(), True),\n",
                "    StructField(\"year\", ShortType(), True),\n",
                "    StructField(\"d\", StringType(), True),\n",
                "    StructField(\"event_name_1\", StringType(), True),\n",
                "    StructField(\"event_type_1\", StringType(), True),\n",
                "    StructField(\"event_name_2\", StringType(), True),\n",
                "    StructField(\"event_type_2\", StringType(), True),\n",
                "    StructField(\"snap_CA\", ByteType(), True),\n",
                "    StructField(\"snap_TX\", ByteType(), True),\n",
                "    StructField(\"snap_WI\", ByteType(), True),\n",
                "])\n",
                "\n",
                "prices_schema = StructType([\n",
                "    StructField(\"store_id\", StringType(), True),\n",
                "    StructField(\"item_id\", StringType(), True),\n",
                "    StructField(\"wm_yr_wk\", ShortType(), True),\n",
                "    StructField(\"sell_price\", FloatType(), True)\n",
                "])\n",
                "\n",
                "print(\"Schemas defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_sales = spark.read.csv(PATH_SALES, header=True, schema=sales_schema)\n",
                "df_cal = spark.read.csv(PATH_CAL, header=True, schema=calendar_schema)\n",
                "df_prices = spark.read.csv(PATH_PRICES, header=True, schema=prices_schema)\n",
                "\n",
                "\n",
                "df_cal.cache()\n",
                "\n",
                "print(f\"Sales rows: {df_sales.count():,}\")\n",
                "print(f\"Calendar rows: {df_cal.count():,}\")\n",
                "print(f\"Prices rows: {df_prices.count():,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Transform - Melt and Join"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "day_cols = [c for c in df_sales.columns if 'd_' in c]\n",
                "id_cols = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
                "\n",
                "stack_expr = f\"stack({len(day_cols)}, \" + \\\n",
                "             \", \".join([f\"'{c}', {c}\" for c in day_cols]) + \\\n",
                "             \") as (d, sales)\"\n",
                "\n",
                "df_melted = df_sales.select(*id_cols, F.expr(stack_expr))\n",
                "print(f\"Melted data will have ~{30490 * len(day_cols):,} rows\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_cal_prep = df_cal.select(\n",
                "    \"d\", \"wm_yr_wk\", \"wday\", \"month\", \"year\", \n",
                "    \"event_name_1\", \"event_type_1\", \"snap_CA\", \"snap_TX\", \"snap_WI\"\n",
                ")\n",
                "\n",
                "\n",
                "df_sales_cal = df_melted.join(F.broadcast(df_cal_prep), on=\"d\", how=\"left\")\n",
                "\n",
                "\n",
                "df_final = df_sales_cal.join(df_prices, on=[\"store_id\", \"item_id\", \"wm_yr_wk\"], how=\"left\")\n",
                "\n",
                "\n",
                "columns = [\"store_id\", \"item_id\", \"wm_yr_wk\", \"d\", \"id\", \"dept_id\", \"cat_id\", \n",
                "           \"state_id\", \"sales\", \"wday\", \"month\", \"year\", \"event_name_1\", \n",
                "           \"event_type_1\", \"snap_CA\", \"snap_TX\", \"snap_WI\", \"sell_price\"]\n",
                "\n",
                "df_final = df_final.select(*columns)\n",
                "df_final.printSchema()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Write to Parquet"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if os.path.exists(PARQUET_PATH):\n",
                "    shutil.rmtree(PARQUET_PATH)\n",
                "    print(f\"Cleaned up existing temp files\")\n",
                "\n",
                "print(\"Writing data to Parquet (this is disk-based and memory-safe)...\")\n",
                "df_final.write.parquet(PARQUET_PATH, mode=\"overwrite\")\n",
                "print(\"Parquet write complete!\")\n",
                "\n",
                "spark.stop()\n",
                "print(\"Spark stopped - memory freed for SQLite writing\")\n",
                "gc.collect()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Read Parquet and Write to SQLite (Chunk by Chunk)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "parquet_files = [os.path.join(PARQUET_PATH, f) for f in os.listdir(PARQUET_PATH) if f.endswith('.parquet')]\n",
                "print(f\"Found {len(parquet_files)} parquet files to process\")\n",
                "\n",
                "# Open SQLite connection\n",
                "conn = sqlite3.connect(DB_PATH)\n",
                "total_rows = 0\n",
                "\n",
                "for i, pq_file in enumerate(parquet_files, 1):\n",
                "    print(f\"\\n[{i}/{len(parquet_files)}] Processing: {os.path.basename(pq_file)}\")\n",
                "    \n",
                "    try:\n",
                "        # Read parquet file\n",
                "        table = pq.read_table(pq_file)\n",
                "        df = table.to_pandas()\n",
                "        row_count = len(df)\n",
                "        \n",
                "        # Write to SQLite in chunks\n",
                "        df.to_sql('sales_data', conn, if_exists='append', index=False, chunksize=50000)\n",
                "        total_rows += row_count\n",
                "        \n",
                "        print(f\"   -> Written {row_count:,} rows (Total: {total_rows:,})\")\n",
                "        \n",
                "        # Free memory\n",
                "        del df, table\n",
                "        gc.collect()\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"   -> ERROR: {e}\")\n",
                "\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(f\"Total rows written: {total_rows:,}\")\n",
                "print(f\"{'='*50}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Verify Data and Create Indexes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cursor = conn.cursor()\n",
                "\n",
                "# Verify row count\n",
                "cursor.execute(\"SELECT COUNT(*) FROM sales_data\")\n",
                "db_rows = cursor.fetchone()[0]\n",
                "print(f\"Total rows in database: {db_rows:,}\")\n",
                "\n",
                "\n",
                "cursor.execute(\"\"\"\n",
                "    SELECT store_id, item_id, d, COUNT(*) as cnt \n",
                "    FROM sales_data \n",
                "    GROUP BY store_id, item_id, d \n",
                "    HAVING cnt > 1 \n",
                "    LIMIT 5\n",
                "\"\"\")\n",
                "duplicates = cursor.fetchall()\n",
                "if duplicates:\n",
                "    print(f\"WARNING: Found duplicates: {duplicates}\")\n",
                "else:\n",
                "    print(\"No duplicates found - data is unique!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cursor.execute(\"SELECT * FROM sales_data LIMIT 5\")\n",
                "sample = cursor.fetchall()\n",
                "col_names = [desc[0] for desc in cursor.description]\n",
                "\n",
                "print(f\"Columns: {col_names}\")\n",
                "print(f\"\\nSample rows:\")\n",
                "for row in sample:\n",
                "    print(row)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Creating indexes...\")\n",
                "cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_store_item ON sales_data(store_id, item_id)\")\n",
                "cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_d ON sales_data(d)\")\n",
                "cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_year_month ON sales_data(year, month)\")\n",
                "cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_state ON sales_data(state_id)\")\n",
                "conn.commit()\n",
                "print(\"Indexes created!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Cleanup and Finish"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "conn.close()\n",
                "\n",
                "\n",
                "print(\"Cleaning up temporary Parquet files...\")\n",
                "if os.path.exists(PARQUET_PATH):\n",
                "    shutil.rmtree(PARQUET_PATH)\n",
                "    print(\"  -> Temp files deleted\")\n",
                "\n",
                "\n",
                "db_size_mb = os.path.getsize(DB_PATH) / (1024 * 1024)\n",
                "print(f\"\\nDatabase file size: {db_size_mb:.2f} MB\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"ALL DONE! Database is ready.\")\n",
                "print(f\"Location: {DB_PATH}\")\n",
                "print(\"=\"*50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## (Optional) Download Database"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download the database file to your local machine\n",
                "from google.colab import files\n",
                "files.download(DB_PATH)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## (Optional) Quick Test Query"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "conn = sqlite3.connect(DB_PATH)\n",
                "test_df = pd.read_sql_query(\"\"\"\n",
                "    SELECT store_id, COUNT(*) as row_count, SUM(sales) as total_sales\n",
                "    FROM sales_data\n",
                "    GROUP BY store_id\n",
                "    ORDER BY store_id\n",
                "\"\"\", conn)\n",
                "conn.close()\n",
                "\n",
                "print(\"Sales summary by store:\")\n",
                "print(test_df)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "pyspark_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
