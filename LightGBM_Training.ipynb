{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LightGBM Training with PySpark & GPU\n",
                "\n",
                "This notebook trains a LightGBM model on the M5 Sales data using:\n",
                "- **PySpark**: For efficient data loading and memory management.\n",
                "- **LightGBM**: For gradient boosting.\n",
                "- **GPU (RTX 3050)**: To accelerate training and prevent system crashes.\n",
                "\n",
                "**NOTE**: If you encounter `ConnectionRefusedError`, please **Restart the Kernel** to clear the previous crashed Spark session."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING: Using incubator modules: jdk.incubator.vector\n",
                        "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
                        "25/12/14 13:52:27 WARN Utils: Your hostname, parrot, resolves to a loopback address: 127.0.1.1; using 192.168.1.12 instead (on interface wlo1)\n",
                        "25/12/14 13:52:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
                        "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
                        "Setting default log level to \"WARN\".\n",
                        "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
                        "25/12/14 13:52:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Spark Version: 4.0.1\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import gc\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import lightgbm as lgb\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql import functions as F\n",
                "\n",
                "# 1. Initialize Spark Session with Memory Limits\n",
                "# Disabling Arrow optimization ('spark.sql.execution.arrow.pyspark.enabled': 'false') \n",
                "# because it caused JVM crashes/memory leaks on this specific setup.\n",
                "# Reduced memory to 2g to be safer on laptops.\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"M5_LGBM_Training\") \\\n",
                "    .config(\"spark.driver.memory\", \"2g\") \\\n",
                "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
                "    .getOrCreate()\n",
                "\n",
                "print(f\"Spark Version: {spark.version}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Data from Parquet\n",
                "Loading the optimized dataset created in the ETL step."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total Rows: 58,327,370\n",
                        "root\n",
                        " |-- sell_price: float (nullable = true)\n",
                        " |-- wm_yr_wk: integer (nullable = true)\n",
                        " |-- sales: integer (nullable = true)\n",
                        " |-- wday: integer (nullable = true)\n",
                        " |-- month: integer (nullable = true)\n",
                        " |-- year: integer (nullable = true)\n",
                        " |-- snap_CA: integer (nullable = true)\n",
                        " |-- snap_TX: integer (nullable = true)\n",
                        " |-- snap_WI: integer (nullable = true)\n",
                        " |-- store_id: string (nullable = true)\n",
                        " |-- item_id: string (nullable = true)\n",
                        " |-- d: string (nullable = true)\n",
                        " |-- id: string (nullable = true)\n",
                        " |-- dept_id: string (nullable = true)\n",
                        " |-- cat_id: string (nullable = true)\n",
                        " |-- state_id: string (nullable = true)\n",
                        " |-- event_name_1: string (nullable = true)\n",
                        " |-- event_type_1: string (nullable = true)\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Path to the optimized parquet file\n",
                "# Adjust this path if 'final_optimized.parquet' is in a different subdirectory\n",
                "DATA_PATH = \"ETL Process/final_optimized.parquet\"\n",
                "MODEL_DIR = \"models/LightGBM\"\n",
                "os.makedirs(MODEL_DIR, exist_ok=True)\n",
                "\n",
                "# Read Parquet file\n",
                "df_spark = spark.read.parquet(DATA_PATH)\n",
                "\n",
                "print(f\"Total Rows: {df_spark.count():,}\")\n",
                "df_spark.printSchema()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Preparation\n",
                "Converting from Spark DataFrame to Pandas for LightGBM. \n",
                "**Memory Optimization**: We explicitly cast to suitable types to save RAM."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Converting to Pandas (this might take a minute)...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 4:==>                                                      (1 + 19) / 20]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[7.191s][warning][gc,alloc] Executor task launch worker for task 4.0 in stage 4.0 (TID 26): Retried waiting for GCLocker too often allocating 15936 words\n",
                        "[7.396s][warning][gc,alloc] Executor task launch worker for task 0.0 in stage 4.0 (TID 22): Retried waiting for GCLocker too often allocating 131074 words\n",
                        "[7.402s][warning][gc,alloc] Executor task launch worker for task 17.0 in stage 4.0 (TID 39): Retried waiting for GCLocker too often allocating 131074 words\n",
                        "[7.408s][warning][gc,alloc] Executor task launch worker for task 9.0 in stage 4.0 (TID 31): Retried waiting for GCLocker too often allocating 17641 words\n",
                        "[7.408s][warning][gc,alloc] Executor task launch worker for task 3.0 in stage 4.0 (TID 25): Retried waiting for GCLocker too often allocating 14125 words\n",
                        "[7.408s][warning][gc,alloc] Executor task launch worker for task 13.0 in stage 4.0 (TID 35): Retried waiting for GCLocker too often allocating 16988 words\n",
                        "[7.408s][warning][gc,alloc] Executor task launch worker for task 16.0 in stage 4.0 (TID 38): Retried waiting for GCLocker too often allocating 14356 words\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "25/12/14 13:52:34 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 22)\n",
                        "java.lang.OutOfMemoryError: Java heap space\n",
                        "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
                        "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3817/0x00007fbe31044000.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
                        "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:405)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3783/0x00007fbe31043a60.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD$$Lambda$3383/0x00007fbe31034c10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
                        "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2215/0x00007fbe30c25f10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "25/12/14 13:52:34 ERROR Executor: Exception in task 17.0 in stage 4.0 (TID 39)\n",
                        "java.lang.OutOfMemoryError: Java heap space\n",
                        "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
                        "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3817/0x00007fbe31044000.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
                        "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:405)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3783/0x00007fbe31043a60.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD$$Lambda$3383/0x00007fbe31034c10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
                        "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2215/0x00007fbe30c25f10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "25/12/14 13:52:34 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 4.0 (TID 22),5,main]\n",
                        "java.lang.OutOfMemoryError: Java heap space\n",
                        "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
                        "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3817/0x00007fbe31044000.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
                        "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:405)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3783/0x00007fbe31043a60.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD$$Lambda$3383/0x00007fbe31034c10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
                        "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2215/0x00007fbe30c25f10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "25/12/14 13:52:34 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 17.0 in stage 4.0 (TID 39),5,main]\n",
                        "java.lang.OutOfMemoryError: Java heap space\n",
                        "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
                        "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3817/0x00007fbe31044000.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
                        "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:405)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3783/0x00007fbe31043a60.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD$$Lambda$3383/0x00007fbe31034c10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
                        "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2215/0x00007fbe30c25f10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "25/12/14 13:52:34 WARN TaskSetManager: Lost task 17.0 in stage 4.0 (TID 39) (192.168.1.12 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
                        "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
                        "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3817/0x00007fbe31044000.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
                        "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:405)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3783/0x00007fbe31043a60.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD$$Lambda$3383/0x00007fbe31034c10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
                        "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2215/0x00007fbe30c25f10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "\n",
                        "25/12/14 13:52:34 ERROR TaskSetManager: Task 17 in stage 4.0 failed 1 times; aborting job\n",
                        "25/12/14 13:52:34 WARN TaskSetManager: Lost task 1.0 in stage 4.0 (TID 23) (192.168.1.12 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 17 in stage 4.0 failed 1 times, most recent failure: Lost task 17.0 in stage 4.0 (TID 39) (192.168.1.12 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
                        "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
                        "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3817/0x00007fbe31044000.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
                        "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:405)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3783/0x00007fbe31043a60.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD$$Lambda$3383/0x00007fbe31034c10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
                        "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2215/0x00007fbe30c25f10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "\n",
                        "Driver stacktrace:)\n",
                        "25/12/14 13:52:34 WARN TaskSetManager: Lost task 12.0 in stage 4.0 (TID 34) (192.168.1.12 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 17 in stage 4.0 failed 1 times, most recent failure: Lost task 17.0 in stage 4.0 (TID 39) (192.168.1.12 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
                        "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
                        "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3817/0x00007fbe31044000.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
                        "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:405)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3783/0x00007fbe31043a60.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD$$Lambda$3383/0x00007fbe31034c10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
                        "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2215/0x00007fbe30c25f10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "\n",
                        "Driver stacktrace:)\n",
                        "25/12/14 13:52:34 WARN TaskSetManager: Lost task 7.0 in stage 4.0 (TID 29) (192.168.1.12 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 17 in stage 4.0 failed 1 times, most recent failure: Lost task 17.0 in stage 4.0 (TID 39) (192.168.1.12 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
                        "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
                        "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3817/0x00007fbe31044000.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
                        "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:405)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3783/0x00007fbe31043a60.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD$$Lambda$3383/0x00007fbe31034c10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
                        "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2215/0x00007fbe30c25f10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "\n",
                        "Driver stacktrace:)\n",
                        "25/12/14 13:52:34 WARN TaskSetManager: Lost task 3.0 in stage 4.0 (TID 25) (192.168.1.12 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 17 in stage 4.0 failed 1 times, most recent failure: Lost task 17.0 in stage 4.0 (TID 39) (192.168.1.12 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
                        "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
                        "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3817/0x00007fbe31044000.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
                        "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:405)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3783/0x00007fbe31043a60.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD$$Lambda$3383/0x00007fbe31034c10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
                        "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2215/0x00007fbe30c25f10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "\n",
                        "Driver stacktrace:)\n",
                        "25/12/14 13:52:34 WARN TaskSetManager: Lost task 18.0 in stage 4.0 (TID 40) (192.168.1.12 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 17 in stage 4.0 failed 1 times, most recent failure: Lost task 17.0 in stage 4.0 (TID 39) (192.168.1.12 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
                        "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
                        "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3817/0x00007fbe31044000.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
                        "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:405)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3783/0x00007fbe31043a60.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD$$Lambda$3383/0x00007fbe31034c10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
                        "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2215/0x00007fbe30c25f10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "\n",
                        "Driver stacktrace:)\n",
                        "25/12/14 13:52:34 WARN TaskSetManager: Lost task 6.0 in stage 4.0 (TID 28) (192.168.1.12 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 17 in stage 4.0 failed 1 times, most recent failure: Lost task 17.0 in stage 4.0 (TID 39) (192.168.1.12 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
                        "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
                        "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3817/0x00007fbe31044000.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
                        "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:405)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3783/0x00007fbe31043a60.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD$$Lambda$3383/0x00007fbe31034c10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
                        "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2215/0x00007fbe30c25f10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "\n",
                        "Driver stacktrace:)\n",
                        "25/12/14 13:52:34 WARN TaskSetManager: Lost task 15.0 in stage 4.0 (TID 37) (192.168.1.12 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 17 in stage 4.0 failed 1 times, most recent failure: Lost task 17.0 in stage 4.0 (TID 39) (192.168.1.12 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
                        "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
                        "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3817/0x00007fbe31044000.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
                        "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:405)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3783/0x00007fbe31043a60.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD$$Lambda$3383/0x00007fbe31034c10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
                        "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2215/0x00007fbe30c25f10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "\n",
                        "Driver stacktrace:)\n",
                        "25/12/14 13:52:34 WARN TaskSetManager: Lost task 10.0 in stage 4.0 (TID 32) (192.168.1.12 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 17 in stage 4.0 failed 1 times, most recent failure: Lost task 17.0 in stage 4.0 (TID 39) (192.168.1.12 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
                        "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
                        "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3817/0x00007fbe31044000.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
                        "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:405)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3783/0x00007fbe31043a60.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD$$Lambda$3383/0x00007fbe31034c10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
                        "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2215/0x00007fbe30c25f10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "\n",
                        "Driver stacktrace:)\n",
                        "25/12/14 13:52:34 WARN TaskSetManager: Lost task 2.0 in stage 4.0 (TID 24) (192.168.1.12 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 17 in stage 4.0 failed 1 times, most recent failure: Lost task 17.0 in stage 4.0 (TID 39) (192.168.1.12 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
                        "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
                        "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3817/0x00007fbe31044000.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
                        "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:405)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3783/0x00007fbe31043a60.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD$$Lambda$3383/0x00007fbe31034c10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
                        "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2215/0x00007fbe30c25f10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "\n",
                        "Driver stacktrace:)\n",
                        "25/12/14 13:52:34 WARN TaskSetManager: Lost task 5.0 in stage 4.0 (TID 27) (192.168.1.12 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 17 in stage 4.0 failed 1 times, most recent failure: Lost task 17.0 in stage 4.0 (TID 39) (192.168.1.12 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
                        "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
                        "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3817/0x00007fbe31044000.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
                        "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:405)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3783/0x00007fbe31043a60.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD$$Lambda$3383/0x00007fbe31034c10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
                        "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2215/0x00007fbe30c25f10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "\n",
                        "Driver stacktrace:)\n",
                        "25/12/14 13:52:34 WARN TaskSetManager: Lost task 16.0 in stage 4.0 (TID 38) (192.168.1.12 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 17 in stage 4.0 failed 1 times, most recent failure: Lost task 17.0 in stage 4.0 (TID 39) (192.168.1.12 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
                        "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
                        "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3817/0x00007fbe31044000.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
                        "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:405)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3783/0x00007fbe31043a60.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD$$Lambda$3383/0x00007fbe31034c10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
                        "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2215/0x00007fbe30c25f10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "\n",
                        "Driver stacktrace:)\n",
                        "25/12/14 13:52:34 WARN TaskSetManager: Lost task 14.0 in stage 4.0 (TID 36) (192.168.1.12 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 17 in stage 4.0 failed 1 times, most recent failure: Lost task 17.0 in stage 4.0 (TID 39) (192.168.1.12 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
                        "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
                        "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3817/0x00007fbe31044000.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
                        "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:405)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3783/0x00007fbe31043a60.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD$$Lambda$3383/0x00007fbe31034c10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
                        "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2215/0x00007fbe30c25f10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "\n",
                        "Driver stacktrace:)\n",
                        "25/12/14 13:52:34 WARN TaskSetManager: Lost task 9.0 in stage 4.0 (TID 31) (192.168.1.12 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 17 in stage 4.0 failed 1 times, most recent failure: Lost task 17.0 in stage 4.0 (TID 39) (192.168.1.12 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
                        "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
                        "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3817/0x00007fbe31044000.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
                        "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:405)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3783/0x00007fbe31043a60.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD$$Lambda$3383/0x00007fbe31034c10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
                        "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2215/0x00007fbe30c25f10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "\n",
                        "Driver stacktrace:)\n",
                        "25/12/14 13:52:34 WARN TaskSetManager: Lost task 8.0 in stage 4.0 (TID 30) (192.168.1.12 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 17 in stage 4.0 failed 1 times, most recent failure: Lost task 17.0 in stage 4.0 (TID 39) (192.168.1.12 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
                        "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
                        "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3817/0x00007fbe31044000.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
                        "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:405)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3783/0x00007fbe31043a60.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD$$Lambda$3383/0x00007fbe31034c10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
                        "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2215/0x00007fbe30c25f10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "\n",
                        "Driver stacktrace:)\n",
                        "25/12/14 13:52:34 WARN TaskSetManager: Lost task 11.0 in stage 4.0 (TID 33) (192.168.1.12 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 17 in stage 4.0 failed 1 times, most recent failure: Lost task 17.0 in stage 4.0 (TID 39) (192.168.1.12 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
                        "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
                        "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:382)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3817/0x00007fbe31044000.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
                        "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
                        "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
                        "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:405)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3783/0x00007fbe31043a60.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
                        "\tat org.apache.spark.rdd.RDD$$Lambda$3383/0x00007fbe31034c10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
                        "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2215/0x00007fbe30c25f10.apply(Unknown Source)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
                        "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "\n",
                        "Driver stacktrace:)\n",
                        "25/12/14 13:52:34 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
                        "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$3966/0x00007fbe31014850@4c1561cc rejected from java.util.concurrent.ThreadPoolExecutor@482aec96[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 40]\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
                        "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:140)\n",
                        "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:813)\n",
                        "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:786)\n",
                        "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:73)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:116)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "25/12/14 13:52:34 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
                        "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$3966/0x00007fbe31014850@3c6f21bc rejected from java.util.concurrent.ThreadPoolExecutor@482aec96[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 40]\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
                        "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:140)\n",
                        "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:813)\n",
                        "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:786)\n",
                        "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:73)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:116)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
                    ]
                },
                {
                    "ename": "ConnectionRefusedError",
                    "evalue": "[Errno 111] Connection refused",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
                        "\u001b[31m<class 'str'>\u001b[39m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
                        "\nDuring handling of the above exception, another exception occurred:\n",
                        "\u001b[31mPy4JNetworkError\u001b[39m                          Traceback (most recent call last)",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/py4j/clientserver.py:540\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer.strip() == \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[32m    541\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAnswer from Java side is empty\u001b[39m\u001b[33m\"\u001b[39m, when=proto.EMPTY_RESPONSE)\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer.startswith(proto.RETURN_MESSAGE):\n",
                        "\u001b[31mPy4JNetworkError\u001b[39m: Answer from Java side is empty",
                        "\nDuring handling of the above exception, another exception occurred:\n",
                        "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/pyspark/sql/classic/dataframe.py:443\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m._sc):\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m     sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:284\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     converted = \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjava_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m         \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m         \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:260\u001b[39m, in \u001b[36mconvert_exception\u001b[39m\u001b[34m(e)\u001b[39m\n\u001b[32m    258\u001b[39m stacktrace: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mgetattr\u001b[39m(jvm, \u001b[33m\"\u001b[39m\u001b[33morg.apache.spark.util.Utils\u001b[39m\u001b[33m\"\u001b[39m).exceptionString(e)\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     \u001b[43mis_instance_of\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.apache.spark.api.python.PythonException\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m     \u001b[38;5;66;03m# To make sure this only catches Python UDFs.\u001b[39;00m\n\u001b[32m    262\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    263\u001b[39m         \u001b[38;5;28mmap\u001b[39m(\n\u001b[32m    264\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m v: \u001b[33m\"\u001b[39m\u001b[33morg.apache.spark.sql.execution.python\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m v.toString(), c.getStackTrace()\n\u001b[32m    265\u001b[39m         )\n\u001b[32m    266\u001b[39m     )\n\u001b[32m    267\u001b[39m ):\n\u001b[32m    268\u001b[39m     msg = (\n\u001b[32m    269\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  An exception was thrown from the Python worker. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    270\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease see the stack trace below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % c.getMessage()\n\u001b[32m    271\u001b[39m     )\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:464\u001b[39m, in \u001b[36mis_instance_of\u001b[39m\u001b[34m(gateway, java_object, java_class)\u001b[39m\n\u001b[32m    461\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    462\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mjava_class must be a string, a JavaClass, or a JavaObject\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgateway\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpy4j\u001b[49m.reflection.TypeUtil.isInstanceOf(\n\u001b[32m    465\u001b[39m     param, java_object)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1752\u001b[39m, in \u001b[36mJVMView.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[32m-> \u001b[39m\u001b[32m1752\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1753\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m   1754\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m   1755\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1756\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer == proto.SUCCESS_PACKAGE:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1057\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1056\u001b[39m     logging.info(\u001b[33m\"\u001b[39m\u001b[33mException while sending command.\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1057\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m   called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m   :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    288\u001b[39m connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m     \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m     \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused",
                        "\nDuring handling of the above exception, another exception occurred:\n",
                        "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Convert to Pandas (Standard conversion, safer than Arrow for large data on constrained RAM)\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mConverting to Pandas (this might take a minute)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df = \u001b[43mdf_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Optimize types to save memory\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df.columns:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/pyspark/sql/classic/dataframe.py:1792\u001b[39m, in \u001b[36mDataFrame.toPandas\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtoPandas\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mPandasDataFrameLike\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1792\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPandasConversionMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:197\u001b[39m, in \u001b[36mPandasConversionMixin.toPandas\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    194\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m rows = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) > \u001b[32m0\u001b[39m:\n\u001b[32m    199\u001b[39m     pdf = pd.DataFrame.from_records(\n\u001b[32m    200\u001b[39m         rows, index=\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns=\u001b[38;5;28mself\u001b[39m.columns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    201\u001b[39m     )\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/pyspark/sql/classic/dataframe.py:442\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcollect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List[Row]:\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSCCallSiteSync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock_info\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/pyspark/traceback_utils.py:81\u001b[39m, in \u001b[36mSCCallSiteSync.__exit__\u001b[39m\u001b[34m(self, type, value, tb)\u001b[39m\n\u001b[32m     79\u001b[39m SCCallSiteSync._spark_stack_depth -= \u001b[32m1\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m SCCallSiteSync._spark_stack_depth == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jsc\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetCallSite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1361\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1354\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m return_value = get_return_value(\n\u001b[32m   1363\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DevProjects/M5-Sales-Forecasting/.venv/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
                        "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused"
                    ]
                }
            ],
            "source": [
                "# Define features and target\n",
                "# Assuming 'sales' is the target\n",
                "TARGET = 'sales'\n",
                "\n",
                "# Convert to Pandas (Standard conversion, safer than Arrow for large data on constrained RAM)\n",
                "print(\"Converting to Pandas (this might take a minute)...\")\n",
                "df = df_spark.toPandas()\n",
                "\n",
                "# Optimize types to save memory\n",
                "for col in df.columns:\n",
                "    if df[col].dtype == 'float64':\n",
                "        df[col] = df[col].astype('float32')\n",
                "    if df[col].dtype == 'int64':\n",
                "        # Check range to see if int32/int16 suffices\n",
                "        if df[col].max() < 2147483647:\n",
                "            df[col] = df[col].astype('int32')\n",
                "\n",
                "print(\"Data loaded into Pandas via Spark.\")\n",
                "print(df.info())\n",
                "\n",
                "# Free up Spark memory\n",
                "spark.stop()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. LightGBM Training (GPU Optimized)\n",
                "Configuring LGBM parameters for RTX 3050."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare Dataset\n",
                "X = df.drop(columns=[TARGET])\n",
                "y = df[TARGET]\n",
                "\n",
                "# Identify categorical features automatically or manually\n",
                "# Common M5 columns: item_id, dept_id, cat_id, store_id, state_id\n",
                "cat_feats = [c for c in X.columns if c in [\n",
                "    'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', \n",
                "    'year', 'month', 'wday', 'event_name_1', 'event_type_1', \n",
                "    'event_name_2', 'event_type_2'\n",
                "]]\n",
                "\n",
                "print(f\"Categorical Features: {cat_feats}\")\n",
                "\n",
                "# Convert object columns to category type for LGBM\n",
                "for c in cat_feats:\n",
                "    if c in X.columns:\n",
                "        X[c] = X[c].astype('category')\n",
                "\n",
                "train_data = lgb.Dataset(X, label=y, categorical_feature=cat_feats)\n",
                "\n",
                "# GPU Configuration\n",
                "params = {\n",
                "    'objective': 'tweedie',\n",
                "    'metric': 'rmse',\n",
                "    'boosting_type': 'gbdt',\n",
                "    'learning_rate': 0.05,\n",
                "    'num_leaves': 63,\n",
                "    'feature_fraction': 0.8,\n",
                "    'bagging_fraction': 0.8,\n",
                "    'bagging_freq': 5,\n",
                "    'verbose': 1,\n",
                "    'n_jobs': -1,\n",
                "    'seed': 42,\n",
                "    \n",
                "    # GPU Parameters for RTX 3050\n",
                "    'device': 'gpu',\n",
                "    'gpu_platform_id': 0,\n",
                "    'gpu_device_id': 0,\n",
                "    # 'gpu_use_dp': False, # Use double precision only if needed (default false is good for speed)\n",
                "    'force_col_wise': True # Optimized for column-wise parallelism\n",
                "}\n",
                "\n",
                "print(\"Starting training with GPU...\")\n",
                "model = lgb.train(\n",
                "    params,\n",
                "    train_data,\n",
                "    num_boost_round=1000,\n",
                "    valid_sets=[train_data],\n",
                "    valid_names=['train'],\n",
                "    callbacks=[\n",
                "        lgb.early_stopping(stopping_rounds=50),\n",
                "        lgb.log_evaluation(period=50)\n",
                "    ]\n",
                ")\n",
                "\n",
                "print(\"Training completed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Save Model and Feature Importance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save model\n",
                "model_path = os.path.join(MODEL_DIR, 'lgb_gpu_model.txt')\n",
                "model.save_model(model_path)\n",
                "print(f\"Model saved to {model_path}\")\n",
                "\n",
                "# Feature Importance\n",
                "importance = pd.DataFrame({\n",
                "    'Feature': model.feature_name(),\n",
                "    'Importance': model.feature_importance(importance_type='gain')\n",
                "}).sort_values(by='Importance', ascending=False)\n",
                "\n",
                "print(\"\\nTop 10 Features:\")\n",
                "print(importance.head(10))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}